# Copyright (c) 2018-present, Royal Bank of Canada.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataset import Dataset
from params import Params

# Import c√°c m√¥ h√¨nh DE
from de_distmult import DE_DistMult
from de_transe import DE_TransE
from de_simple import DE_SimplE
from de_hake1 import DE_HAKE1
from de_hake import DE_HAKE
from de_paire import DE_PaiRE
from de_rotate import DE_RotatE
from de_complex import DE_ComplEx
from de_quatde import DE_QuatDE
from de_convkb import DE_ConvKB

from tester import Tester

class Trainer:
    def __init__(self, dataset, params, model_name):
        instance_gen = globals()[model_name]
        self.model_name = model_name

        # üîß Kh·ªüi t·∫°o model v√† chuy·ªÉn sang GPU ƒë√∫ng c√°ch
        self.model = instance_gen(dataset=dataset, params=params)  # t·∫°o m√¥ h√¨nh
        self.model = self.model.to('cuda')                         # chuy·ªÉn to√†n b·ªô model sang GPU
        self.model = nn.DataParallel(self.model)                   # b·ªçc b·∫±ng DataParallel (n·∫øu c√≥ nhi·ªÅu GPU)

        self.dataset = dataset
        self.params = params

    def train(self, early_stop=False):
        self.model.train()
        num_params = sum(p.numel() for p in self.model.parameters())
        print(f"üî¢ S·ªë l∆∞·ª£ng tham s·ªë c·ªßa m√¥ h√¨nh: {num_params:,}")

        optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=self.params.lr, 
            weight_decay=self.params.reg_lambda
        )

        loss_f = nn.CrossEntropyLoss()

        total_start_time = time.time()  # ‚è±Ô∏è B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian t·ªïng

        for epoch in range(1, self.params.ne + 1):
            epoch_start_time = time.time()

            last_batch = False
            total_loss = 0.0

            while not last_batch:
                optimizer.zero_grad()

                heads, rels, tails, years, months, days = self.dataset.nextBatch(
                    self.params.bsize, neg_ratio=self.params.neg_ratio
                )
                last_batch = self.dataset.wasLastBatch()

                # forward
                scores = self.model(heads, rels, tails, years, months, days)

                num_examples = int(heads.shape[0] / (1 + self.params.neg_ratio))
                scores = scores.view(num_examples, self.params.neg_ratio + 1)

                # ‚ö†Ô∏è label ƒë√∫ng l√† v·ªã tr√≠ 0 (positive sample)
                labels = torch.zeros(num_examples).long().cuda()

                loss = loss_f(scores, labels)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            epoch_end_time = time.time()
            epoch_duration = epoch_end_time - epoch_start_time

            print(f"‚è±Ô∏è Epoch {epoch}/{self.params.ne} m·∫•t {epoch_duration:.2f} gi√¢y")
            print(f"üìâ Loss epoch {epoch}: {total_loss:.4f} ({self.model_name}, {self.dataset.name})")

            if epoch % self.params.save_each == 0:
                self.saveModel(epoch)

        total_end_time = time.time()
        total_duration = total_end_time - total_start_time
        print(f"\n‚úÖ T·ªïng th·ªùi gian hu·∫•n luy·ªán: {total_duration:.2f} gi√¢y")

    def saveModel(self, chkpnt):
        print("üíæ ƒêang l∆∞u m√¥ h√¨nh...")
        directory = f"models/{self.model_name}/{self.dataset.name}/"
        if not os.path.exists(directory):
            os.makedirs(directory)

        model_path = directory + self.params.str_() + f"_{chkpnt}.chkpnt"
        torch.save(self.model, model_path)
        print(f"‚úÖ ƒê√£ l∆∞u: {model_path}")
